{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bd26fb",
   "metadata": {},
   "source": [
    "# Sagemaker-Huggingface SDK Training Job for Clean Summary Model\n",
    "In this notebook I finetune the initial version of the clean summary model. Whose job will be to summarize why messages marked as innapropriate by the rule adherance classifier were marked so. The caveat to this model is that the summary must itself be appropriate. To accomplish this I used code interpreter to label the toxic comments of the wikipedia toxic comments dataset with clean summaries. The model being fine tuned is T5 which is a seq2seq transformer architecture commonly used for summary.\n",
    "\n",
    "---------\n",
    "For context about the training environment: The sagemaker-huggingface SDK runs a training job specified in the train.py script. The model and hyperparams are specified in the huggingface estimator object. The prepared data is uploaded to an s3 bucket and the URI of each dataset is passed to estimator in the .fit() call. The base T5 model is also upload to s3 in this way to be passed into the training job.\n",
    "\n",
    "https://huggingface.co/docs/sagemaker/train Here is the documentation for aws-huggingface training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34afb824",
   "metadata": {},
   "source": [
    "## Setup Sagemaker Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3273fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0880c",
   "metadata": {},
   "source": [
    "## Huggingface Estimator\n",
    "The huggingface estimator carries out the training script train.py optimized by sagemaker. These arguments will be sent to the training script train.py where the trainer object will be instantiated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e703a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters = {\n",
    "    'epochs': 1,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 16, \n",
    "    'model_name_or_path': 't5-small'\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir='/home/ec2-user/SageMaker/clean_summary_model_training', #path to your training script\n",
    "        instance_type='ml.g4dn.xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.28.1',\n",
    "        pytorch_version='2.0.0',\n",
    "        py_version='py310',\n",
    "        hyperparameters = hyperparameters,\n",
    "        vpc_config={  #for using custom datasets\n",
    "            'Subnets': [\"replaced\"],    #get these from VPC dash\n",
    "            'SecurityGroupIds': ['replaced'] #delete sensitive info before uploading to github\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def20c2a",
   "metadata": {},
   "source": [
    "## Upload datasets to s3 for estimator to access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5b9a6",
   "metadata": {},
   "source": [
    "I was having issues with the deep learning container losing internet access so I opted to upload datasets to s3 and pass their URIs into the estimator. This means that the train.py script will retrieve them from the s3 bucket before running trainer.train() \n",
    "\n",
    "These next few cells are all related to uploading to s3. If the data is already uploaded these can be disregarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab96d13",
   "metadata": {},
   "source": [
    "The below code creates a new s3 bucket to store our datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3', region_name='us-east-2') #us-east-2 is specific to my specs. May need to be changed if used\n",
    "bucket_name = 'hambart-training'\n",
    "\n",
    "# Create a new bucket in the us-east-2 region\n",
    "s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': 'us-east-2'})\n",
    "\n",
    "# Check the location of an existing bucket\n",
    "response = s3.get_bucket_location(Bucket=bucket_name)\n",
    "bucket_location = response['LocationConstraint']\n",
    "print('Bucket location:', bucket_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4906cfd",
   "metadata": {},
   "source": [
    "The datasets have already been tokenized and were saved as datasets.Dataset objects in the RAC_dataset_tokenization.ipynb notebook.\n",
    "\n",
    "Below we upload the files for each dataset into seperate directories created in the s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68240d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'hambart-training'\n",
    "\n",
    "base_path = '/home/ec2-user/SageMaker/clean_summary_model_training/'\n",
    "dataset_paths = [os.path.join(base_path, 'summary_train_dataset/'),\n",
    "                 os.path.join(base_path, 'summary_test_dataset/')]\n",
    "\n",
    "def upload_directory_to_s3(directory_path, s3_bucket, s3_key_prefix=''):\n",
    "    '''\n",
    "    uploads dataset folders to new dir in s3 bucket \n",
    "    for estimator to access. dir names are that which\n",
    "    they are pulled from.\n",
    "    '''\n",
    "    directory_name = os.path.basename(directory_path.rstrip('/'))  # Ensure no trailing slash\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            s3_key = os.path.join(s3_key_prefix, directory_name, os.path.relpath(file_path, directory_path))\n",
    "            print(f\"Uploading {file_path} to s3://{s3_bucket.name}/{s3_key}\")\n",
    "            s3_bucket.Object(s3_key).upload_file(Filename=file_path)\n",
    "\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "for folder_path in dataset_paths:\n",
    "    upload_directory_to_s3(folder_path, bucket)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225cd13",
   "metadata": {},
   "source": [
    "Below we create and print the s3 buck uris to each of the dataset dirs. These will be passed to the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd21242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://hambart-training/summary_train_dataset\n",
      "s3://hambart-training/summary_test_dataset\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'hambart-training'\n",
    "\n",
    "train_dataset_uri = 's3://{}/{}'.format(bucket_name, 'summary_train_dataset')\n",
    "test_dataset_uri = 's3://{}/{}'.format(bucket_name, 'summary_test_dataset')\n",
    "\n",
    "\n",
    "print(train_dataset_uri, '\\n', test_dataset_uri, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93b43d",
   "metadata": {},
   "source": [
    "## Upload base model to s3\n",
    "We also need to upload the distilbert base uncased model to s3 and pass it into the training job. \n",
    "\n",
    "Below we download it and save it to this notebooks s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3c7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "\n",
    "# Load the configuration\n",
    "config = T5Config.from_pretrained(\"t5-small\")  \n",
    "\n",
    "# Load the T5 model for conditional generation (summarization)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", config=config)\n",
    "\n",
    "# Save the model in order to upload it to s3\n",
    "model.save_pretrained(\"./t5-small-summarization-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512fb04",
   "metadata": {},
   "source": [
    "Below we upload the model to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the directory to be created in the bucket\n",
    "directory_name = \"t5-small-summarization-model\"\n",
    "\n",
    "# Upload the entire directory\n",
    "for root, dirs, files in os.walk(\"./t5-small-summarization-model\"):\n",
    "    for filename in files:\n",
    "        local_path = os.path.join(root, filename)\n",
    "        s3_path = os.path.join(directory_name, os.path.relpath(local_path, \"./t5-small-summarization-model\"))\n",
    "        \n",
    "        bucket.upload_file(local_path, s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0bf44",
   "metadata": {},
   "source": [
    "## Fit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e22e28",
   "metadata": {},
   "source": [
    "Calling the .fit() method on the hugging face estimator will carry out the training specified in train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a270b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-08-22-15-40-20-582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 15:46:59 Starting - Starting the training job...\n",
      "2023-08-22 15:47:14 Starting - Preparing the instances for training......\n",
      "2023-08-22 15:48:13 Downloading - Downloading input data...\n",
      "2023-08-22 15:48:33 Training - Downloading the training image...................................................\n",
      "2023-08-22 15:57:10 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:21,710 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:21,729 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:21,738 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:21,746 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:26,515 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:26,544 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:26,572 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:26,582 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name_or_path\": \"t5-small\",\n",
      "        \"per_device_eval_batch_size\": 16,\n",
      "        \"per_device_train_batch_size\": 16\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-08-22-15-40-20-582\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-110852432151/huggingface-pytorch-training-2023-08-22-15-40-20-582/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name_or_path\":\"t5-small\",\"per_device_eval_batch_size\":16,\"per_device_train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-110852432151/huggingface-pytorch-training-2023-08-22-15-40-20-582/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name_or_path\":\"t5-small\",\"per_device_eval_batch_size\":16,\"per_device_train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-08-22-15-40-20-582\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-110852432151/huggingface-pytorch-training-2023-08-22-15-40-20-582/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name_or_path\",\"t5-small\",\"--per_device_eval_batch_size\",\"16\",\"--per_device_train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=t5-small\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --epochs 1 --model_name_or_path t5-small --per_device_eval_batch_size 16 --per_device_train_batch_size 16\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:26,617 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.5/1.5 MB 38.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting rouge_score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1a9c03c05e92175e7290f03f4abff3f6a4f6013ae0c6ea42ea4efce632d99f99\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge_score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nltk, rouge_score\u001b[0m\n",
      "\u001b[34mSuccessfully installed nltk-3.8.1 rouge_score-0.1.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:35,185 - __main__ - INFO -  loaded train_dataset length is: 10994\u001b[0m\n",
      "\u001b[34m2023-08-22 15:57:35,185 - __main__ - INFO -  loaded test_dataset length is: 2748\u001b[0m\n",
      "\u001b[34m/opt/ml/code/train.py:66: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 5.65kB [00:00, 4.59MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.32k/2.32k [00:00<00:00, 14.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)ve/main/spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:00<00:00, 35.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.39M/1.39M [00:00<00:00, 28.2MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/344 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/344 [00:00<04:20,  1.32it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 2/344 [00:01<03:17,  1.73it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 3/344 [00:01<02:58,  1.92it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 4/344 [00:02<02:48,  2.01it/s]\u001b[0m\n",
      "\u001b[34m1%|â–         | 5/344 [00:02<02:43,  2.07it/s]\u001b[0m\n",
      "\u001b[34m2%|â–         | 6/344 [00:03<02:40,  2.11it/s]\u001b[0m\n",
      "\u001b[34m2%|â–         | 7/344 [00:03<02:38,  2.13it/s]\u001b[0m\n",
      "\u001b[34m2%|â–         | 8/344 [00:03<02:36,  2.15it/s]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 9/344 [00:04<02:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 10/344 [00:04<02:33,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 11/344 [00:05<02:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m3%|â–Ž         | 12/344 [00:05<02:32,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|â–         | 13/344 [00:06<02:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|â–         | 14/344 [00:06<02:31,  2.18it/s]\u001b[0m\n",
      "\u001b[34m4%|â–         | 15/344 [00:07<02:30,  2.18it/s]\u001b[0m\n",
      "\u001b[34m5%|â–         | 16/344 [00:07<02:30,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|â–         | 17/344 [00:08<02:29,  2.19it/s]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 18/344 [00:08<02:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 19/344 [00:08<02:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 20/344 [00:09<02:28,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 21/344 [00:09<02:27,  2.19it/s]\u001b[0m\n",
      "\u001b[34m6%|â–‹         | 22/344 [00:10<02:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 23/344 [00:10<02:26,  2.19it/s]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 24/344 [00:11<02:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 25/344 [00:11<02:26,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 26/344 [00:12<02:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 27/344 [00:12<02:25,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 28/344 [00:13<02:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 29/344 [00:13<02:24,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|â–Š         | 30/344 [00:14<02:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 31/344 [00:14<02:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 32/344 [00:14<02:23,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 33/344 [00:15<02:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|â–‰         | 34/344 [00:15<02:22,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 35/344 [00:16<02:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m10%|â–ˆ         | 36/344 [00:16<02:21,  2.18it/s]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 37/344 [00:17<02:20,  2.19it/s]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 38/344 [00:17<02:20,  2.18it/s]\u001b[0m\n",
      "\u001b[34m11%|â–ˆâ–        | 39/344 [00:18<02:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 40/344 [00:18<02:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 41/344 [00:19<02:19,  2.18it/s]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 42/344 [00:19<02:18,  2.17it/s]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–Ž        | 43/344 [00:19<02:18,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 44/344 [00:20<02:17,  2.18it/s]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 45/344 [00:20<02:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m13%|â–ˆâ–Ž        | 46/344 [00:21<02:17,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–Ž        | 47/344 [00:21<02:16,  2.18it/s]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 48/344 [00:22<02:16,  2.17it/s]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 49/344 [00:22<02:16,  2.16it/s]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 50/344 [00:23<02:16,  2.15it/s]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 51/344 [00:23<02:16,  2.15it/s]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 52/344 [00:24<02:15,  2.16it/s]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–Œ        | 53/344 [00:24<02:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 54/344 [00:25<02:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 55/344 [00:25<02:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–‹        | 56/344 [00:25<02:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 57/344 [00:26<02:12,  2.17it/s]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 58/344 [00:26<02:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 59/344 [00:27<02:11,  2.17it/s]\u001b[0m\n",
      "\u001b[34m17%|â–ˆâ–‹        | 60/344 [00:27<02:11,  2.16it/s]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 61/344 [00:28<02:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 62/344 [00:28<02:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 63/344 [00:29<02:10,  2.16it/s]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–Š        | 64/344 [00:29<02:09,  2.16it/s]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 65/344 [00:30<02:09,  2.15it/s]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 66/344 [00:30<02:08,  2.16it/s]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 67/344 [00:31<02:08,  2.16it/s]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–‰        | 68/344 [00:31<02:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 69/344 [00:32<02:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 70/344 [00:32<02:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 71/344 [00:32<02:06,  2.15it/s]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 72/344 [00:33<02:06,  2.15it/s]\u001b[0m\n",
      "\u001b[34m21%|â–ˆâ–ˆ        | 73/344 [00:33<02:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 74/344 [00:34<02:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 75/344 [00:34<02:05,  2.15it/s]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 76/344 [00:35<02:04,  2.15it/s]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 77/344 [00:35<02:04,  2.15it/s]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 78/344 [00:36<02:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 79/344 [00:36<02:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–Ž       | 80/344 [00:37<02:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–Ž       | 81/344 [00:37<02:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 82/344 [00:38<02:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 83/344 [00:38<02:01,  2.15it/s]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 84/344 [00:38<02:01,  2.15it/s]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–       | 85/344 [00:39<02:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 86/344 [00:39<02:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m25%|â–ˆâ–ˆâ–Œ       | 87/344 [00:40<01:59,  2.14it/s]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 88/344 [00:40<01:59,  2.15it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 89/344 [00:41<01:58,  2.14it/s]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 90/344 [00:41<01:58,  2.14it/s]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–‹       | 91/344 [00:42<01:58,  2.14it/s]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 92/344 [00:42<01:57,  2.14it/s]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 93/344 [00:43<01:57,  2.14it/s]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 94/344 [00:43<01:56,  2.14it/s]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 95/344 [00:44<01:56,  2.14it/s]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 96/344 [00:44<01:55,  2.15it/s]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 97/344 [00:45<01:55,  2.14it/s]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 98/344 [00:45<01:54,  2.14it/s]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 99/344 [00:45<01:54,  2.15it/s]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 100/344 [00:46<01:53,  2.14it/s]\u001b[0m\n",
      "\u001b[34m29%|â–ˆâ–ˆâ–‰       | 101/344 [00:46<01:53,  2.14it/s]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 102/344 [00:47<01:52,  2.14it/s]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 103/344 [00:47<01:52,  2.15it/s]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–ˆ       | 104/344 [00:48<01:51,  2.15it/s]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 105/344 [00:48<01:51,  2.14it/s]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 106/344 [00:49<01:51,  2.14it/s]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 107/344 [00:49<01:50,  2.14it/s]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆâ–      | 108/344 [00:50<01:50,  2.14it/s]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 109/344 [00:50<01:49,  2.14it/s]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 110/344 [00:51<01:49,  2.15it/s]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 111/344 [00:51<01:47,  2.17it/s]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 112/344 [00:52<01:47,  2.16it/s]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 113/344 [00:52<01:47,  2.16it/s]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114/344 [00:52<01:46,  2.15it/s]\u001b[0m\n",
      "\u001b[34m33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115/344 [00:53<01:46,  2.15it/s]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–Ž      | 116/344 [00:53<01:45,  2.15it/s]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 117/344 [00:54<01:45,  2.15it/s]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 118/344 [00:54<01:44,  2.16it/s]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 119/344 [00:55<01:44,  2.15it/s]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–      | 120/344 [00:55<01:44,  2.15it/s]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 121/344 [00:56<01:44,  2.14it/s]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 122/344 [00:56<01:43,  2.14it/s]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 123/344 [00:57<01:43,  2.14it/s]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124/344 [00:57<01:42,  2.14it/s]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–‹      | 125/344 [00:58<01:42,  2.14it/s]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 126/344 [00:58<01:41,  2.14it/s]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 127/344 [00:59<01:41,  2.14it/s]\u001b[0m\n",
      "\u001b[34m37%|â–ˆâ–ˆâ–ˆâ–‹      | 128/344 [00:59<01:40,  2.14it/s]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 129/344 [00:59<01:40,  2.14it/s]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 130/344 [01:00<01:40,  2.14it/s]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 131/344 [01:00<01:39,  2.13it/s]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 132/344 [01:01<01:39,  2.14it/s]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–Š      | 133/344 [01:01<01:38,  2.13it/s]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 134/344 [01:02<01:38,  2.13it/s]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 135/344 [01:02<01:37,  2.13it/s]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–‰      | 136/344 [01:03<01:37,  2.14it/s]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–‰      | 137/344 [01:03<01:36,  2.14it/s]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 138/344 [01:04<01:36,  2.13it/s]\u001b[0m\n",
      "\u001b[34m40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 139/344 [01:04<01:36,  2.13it/s]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140/344 [01:05<01:35,  2.13it/s]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141/344 [01:05<01:35,  2.13it/s]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 142/344 [01:06<01:34,  2.13it/s]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 143/344 [01:06<01:34,  2.13it/s]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144/344 [01:07<01:34,  2.13it/s]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145/344 [01:07<01:33,  2.13it/s]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146/344 [01:07<01:33,  2.13it/s]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 147/344 [01:08<01:32,  2.13it/s]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 148/344 [01:08<01:31,  2.13it/s]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149/344 [01:09<01:31,  2.13it/s]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150/344 [01:09<01:31,  2.13it/s]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/344 [01:10<01:30,  2.13it/s]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 152/344 [01:10<01:30,  2.13it/s]\u001b[0m\n",
      "\u001b[34m44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153/344 [01:11<01:29,  2.12it/s]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154/344 [01:11<01:29,  2.12it/s]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 155/344 [01:12<01:28,  2.13it/s]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 156/344 [01:12<01:28,  2.13it/s]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157/344 [01:13<01:28,  2.12it/s]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158/344 [01:13<01:27,  2.13it/s]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159/344 [01:14<01:27,  2.13it/s]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 160/344 [01:14<01:26,  2.13it/s]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 161/344 [01:15<01:26,  2.12it/s]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162/344 [01:15<01:25,  2.12it/s]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163/344 [01:15<01:25,  2.12it/s]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 164/344 [01:16<01:24,  2.12it/s]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 165/344 [01:16<01:24,  2.12it/s]\u001b[0m\n",
      "\u001b[34m48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166/344 [01:17<01:23,  2.12it/s]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167/344 [01:17<01:23,  2.12it/s]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 168/344 [01:18<01:23,  2.12it/s]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 169/344 [01:18<01:22,  2.12it/s]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 170/344 [01:19<01:22,  2.12it/s]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171/344 [01:19<01:21,  2.12it/s]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 172/344 [01:20<01:21,  2.12it/s]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 173/344 [01:20<01:20,  2.12it/s]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 174/344 [01:21<01:20,  2.12it/s]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175/344 [01:21<01:19,  2.12it/s]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176/344 [01:22<01:19,  2.12it/s]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 177/344 [01:22<01:19,  2.10it/s]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 178/344 [01:23<01:19,  2.10it/s]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179/344 [01:23<01:18,  2.10it/s]\u001b[0m\n",
      "\u001b[34m52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180/344 [01:24<01:18,  2.10it/s]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 181/344 [01:24<01:17,  2.11it/s]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 182/344 [01:24<01:16,  2.11it/s]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 183/344 [01:25<01:16,  2.11it/s]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184/344 [01:25<01:15,  2.11it/s]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 185/344 [01:26<01:15,  2.11it/s]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 186/344 [01:26<01:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 187/344 [01:27<01:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188/344 [01:27<01:13,  2.11it/s]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189/344 [01:28<01:13,  2.11it/s]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 190/344 [01:28<01:13,  2.11it/s]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 191/344 [01:29<01:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192/344 [01:29<01:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193/344 [01:30<01:11,  2.11it/s]\u001b[0m\n",
      "\u001b[34m56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 194/344 [01:30<01:11,  2.10it/s]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 195/344 [01:31<01:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 196/344 [01:31<01:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197/344 [01:32<01:09,  2.10it/s]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 198/344 [01:32<01:09,  2.10it/s]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 199/344 [01:33<01:08,  2.10it/s]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 200/344 [01:33<01:08,  2.10it/s]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201/344 [01:33<01:07,  2.10it/s]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202/344 [01:34<01:07,  2.11it/s]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 203/344 [01:34<01:07,  2.10it/s]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 204/344 [01:35<01:06,  2.11it/s]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 205/344 [01:35<01:06,  2.10it/s]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206/344 [01:36<01:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 207/344 [01:36<01:05,  2.11it/s]\u001b[0m\n",
      "\u001b[34m60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 208/344 [01:37<01:04,  2.11it/s]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 209/344 [01:37<01:04,  2.10it/s]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210/344 [01:38<01:03,  2.10it/s]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 211/344 [01:38<01:03,  2.10it/s]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 212/344 [01:39<01:02,  2.11it/s]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 213/344 [01:39<01:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214/344 [01:40<01:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 215/344 [01:40<01:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 216/344 [01:41<01:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 217/344 [01:41<01:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 218/344 [01:42<01:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219/344 [01:42<00:59,  2.09it/s]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 220/344 [01:43<00:59,  2.10it/s]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 221/344 [01:43<00:58,  2.10it/s]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 222/344 [01:43<00:58,  2.10it/s]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223/344 [01:44<00:57,  2.10it/s]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 224/344 [01:44<00:57,  2.10it/s]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 225/344 [01:45<00:56,  2.10it/s]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 226/344 [01:45<00:56,  2.09it/s]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 227/344 [01:46<00:55,  2.09it/s]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 228/344 [01:46<00:55,  2.09it/s]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 229/344 [01:47<00:55,  2.09it/s]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 230/344 [01:47<00:54,  2.09it/s]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 231/344 [01:48<00:54,  2.09it/s]\u001b[0m\n",
      "\u001b[34m67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232/344 [01:48<00:53,  2.10it/s]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 233/344 [01:49<00:53,  2.09it/s]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 234/344 [01:49<00:52,  2.09it/s]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 235/344 [01:50<00:52,  2.09it/s]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236/344 [01:50<00:51,  2.09it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 237/344 [01:51<00:51,  2.09it/s]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 238/344 [01:51<00:50,  2.09it/s]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 239/344 [01:52<00:50,  2.09it/s]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 240/344 [01:52<00:49,  2.09it/s]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 241/344 [01:53<00:49,  2.09it/s]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 242/344 [01:53<00:48,  2.08it/s]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 243/344 [01:54<00:48,  2.09it/s]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 244/344 [01:54<00:47,  2.09it/s]\u001b[0m\n",
      "\u001b[34m71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245/344 [01:54<00:47,  2.08it/s]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 246/344 [01:55<00:46,  2.09it/s]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 247/344 [01:55<00:46,  2.08it/s]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 248/344 [01:56<00:46,  2.08it/s]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249/344 [01:56<00:45,  2.09it/s]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 250/344 [01:57<00:45,  2.09it/s]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 251/344 [01:57<00:44,  2.09it/s]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 252/344 [01:58<00:44,  2.09it/s]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 253/344 [01:58<00:43,  2.08it/s]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 254/344 [01:59<00:43,  2.09it/s]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 255/344 [01:59<00:42,  2.09it/s]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 256/344 [02:00<00:42,  2.08it/s]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 257/344 [02:00<00:41,  2.09it/s]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 258/344 [02:01<00:41,  2.09it/s]\u001b[0m\n",
      "\u001b[34m75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 259/344 [02:01<00:40,  2.08it/s]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 260/344 [02:02<00:40,  2.09it/s]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 261/344 [02:02<00:39,  2.08it/s]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 262/344 [02:03<00:39,  2.08it/s]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 263/344 [02:03<00:38,  2.09it/s]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 264/344 [02:04<00:38,  2.08it/s]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 265/344 [02:04<00:37,  2.08it/s]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 266/344 [02:05<00:37,  2.08it/s]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 267/344 [02:05<00:36,  2.08it/s]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 268/344 [02:06<00:36,  2.08it/s]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 269/344 [02:06<00:36,  2.08it/s]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 270/344 [02:06<00:35,  2.08it/s]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 271/344 [02:07<00:35,  2.08it/s]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 272/344 [02:07<00:34,  2.08it/s]\u001b[0m\n",
      "\u001b[34m79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 273/344 [02:08<00:34,  2.08it/s]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 274/344 [02:08<00:33,  2.08it/s]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 275/344 [02:09<00:33,  2.08it/s]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 276/344 [02:09<00:32,  2.08it/s]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 277/344 [02:10<00:32,  2.09it/s]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 278/344 [02:10<00:31,  2.08it/s]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 279/344 [02:11<00:31,  2.08it/s]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 280/344 [02:11<00:30,  2.08it/s]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 281/344 [02:12<00:30,  2.08it/s]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 282/344 [02:12<00:29,  2.08it/s]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 283/344 [02:13<00:29,  2.08it/s]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 284/344 [02:13<00:28,  2.08it/s]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 285/344 [02:14<00:28,  2.08it/s]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 286/344 [02:14<00:27,  2.08it/s]\u001b[0m\n",
      "\u001b[34m83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 287/344 [02:15<00:27,  2.07it/s]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 288/344 [02:15<00:27,  2.07it/s]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 289/344 [02:16<00:26,  2.08it/s]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 290/344 [02:16<00:25,  2.08it/s]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 291/344 [02:17<00:25,  2.08it/s]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 292/344 [02:17<00:25,  2.08it/s]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 293/344 [02:18<00:24,  2.07it/s]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 294/344 [02:18<00:24,  2.07it/s]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 295/344 [02:19<00:23,  2.08it/s]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 296/344 [02:19<00:23,  2.08it/s]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 297/344 [02:19<00:22,  2.07it/s]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 298/344 [02:20<00:22,  2.08it/s]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 299/344 [02:20<00:21,  2.08it/s]\u001b[0m\n",
      "\u001b[34m87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 300/344 [02:21<00:21,  2.07it/s]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 301/344 [02:21<00:20,  2.07it/s]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 302/344 [02:22<00:20,  2.06it/s]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 303/344 [02:22<00:19,  2.05it/s]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 304/344 [02:23<00:19,  2.04it/s]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 305/344 [02:23<00:19,  2.05it/s]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 306/344 [02:24<00:18,  2.06it/s]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 307/344 [02:24<00:17,  2.07it/s]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 308/344 [02:25<00:17,  2.07it/s]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 309/344 [02:25<00:16,  2.07it/s]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 310/344 [02:26<00:16,  2.07it/s]\u001b[0m\n",
      "\u001b[34m90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 311/344 [02:26<00:15,  2.07it/s]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 312/344 [02:27<00:15,  2.07it/s]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 313/344 [02:27<00:14,  2.07it/s]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 314/344 [02:28<00:14,  2.07it/s]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 315/344 [02:28<00:14,  2.07it/s]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 316/344 [02:29<00:13,  2.07it/s]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 317/344 [02:29<00:13,  2.07it/s]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 318/344 [02:30<00:12,  2.07it/s]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 319/344 [02:30<00:12,  2.07it/s]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 320/344 [02:31<00:11,  2.08it/s]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 321/344 [02:31<00:11,  2.07it/s]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 322/344 [02:32<00:10,  2.07it/s]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323/344 [02:32<00:10,  2.07it/s]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 324/344 [02:33<00:09,  2.07it/s]\u001b[0m\n",
      "\u001b[34m94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 325/344 [02:33<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 326/344 [02:33<00:08,  2.07it/s]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 327/344 [02:34<00:08,  2.07it/s]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 328/344 [02:34<00:07,  2.07it/s]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 329/344 [02:35<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 330/344 [02:35<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 331/344 [02:36<00:06,  2.06it/s]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 332/344 [02:36<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 333/344 [02:37<00:05,  2.07it/s]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 334/344 [02:37<00:04,  2.07it/s]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 335/344 [02:38<00:04,  2.07it/s]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 336/344 [02:38<00:03,  2.07it/s]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 337/344 [02:39<00:03,  2.07it/s]\u001b[0m\n",
      "\u001b[34m98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 338/344 [02:39<00:02,  2.06it/s]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 339/344 [02:40<00:02,  2.06it/s]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 340/344 [02:40<00:01,  2.06it/s]\u001b[0m\n",
      "\n",
      "2023-08-22 16:00:26 Uploading - Uploading generated training model\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 341/344 [02:41<00:01,  2.06it/s]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 342/344 [02:41<00:00,  2.06it/s]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 343/344 [02:42<00:00,  2.07it/s]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [02:42<00:00,  2.36it/s]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 162.5241, 'train_samples_per_second': 67.645, 'train_steps_per_second': 2.117, 'train_loss': 4.5174443444540335, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [02:42<00:00,  2.36it/s]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 344/344 [02:42<00:00,  2.12it/s]\u001b[0m\n",
      "\u001b[34mThe Trainer has finished training at this point\u001b[0m\n",
      "\u001b[34m2023-08-22 16:00:24,589 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-22 16:00:24,589 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-22 16:00:24,590 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-08-22 16:00:57 Completed - Training job completed\n",
      "Training seconds: 764\n",
      "Billable seconds: 764\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': train_dataset_uri, 'test': test_dataset_uri})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
