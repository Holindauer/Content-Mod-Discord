Here is the idea for this model:

I plan to fine tune BERT for an NER task. That task is traditional NER with
the addition of the category of inappropriate/toxic words. What I want out
of this proposed NER task is a model that can identify both inappropriate 
word/concept entities as well as the standard NER entities like person, place,
organization. So, here is the use case: a message is flagged as inappropriate 
by the rule adherance classifier model, it is then passed to the 
NER model which classifies what words are inappropriate as well as any people 
who have been implicated in the statement. This information will then be sent
to the user who sent the inappropriate message as context for why their message 
was flagged. 

So here is a pipeline for building this model. I am going to expand out my thoughts here. 
The classifier I fine tuned was trained on the Wikipedia toxic comments dataset. This dataset
works very well for this topic of toxicity classification in text. I want to augment this 
dataset into an NER dataset for the same examples in the following ways:

I plan to train BERT for standard NER on the CONLL2003 dataset. This is a 
fairly standard thing to do as an NER task and it should yield a model that 
is capable of predicting standard NER tags like person, location, organization. 
Then I will apply this resultant model to the Wikipedia toxic comments dataset 
(on only those rows in it that are considered toxic) to get preliminary labels 
for the tokens in that dataset. This will provide me with a good base for the 
augmented Wikipedia comments set. Then, in order to obtain the toxic entities 
within the examples I will run the toxicity classifier on each word of them. 
That classifier I speak of is good enough that is should be able to determine 
what words contribute to toxic sentences. With this data, I can then construct 
the full wikipedia toxic comment dataset in the correct format.

After the dataset has been made it just becomes a token classification problem. 