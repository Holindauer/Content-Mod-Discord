{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Training for NER labeling job\n",
    "\n",
    "I ran into an issue with my the tokenizer I was using when attempting to use the fine tuned NER model to label the wikipedia toxic comments dataset. That issue was that the tokenizer was not used to the innapropriate words found within the wikipedia dataset, because of that the BertTokenizerFast('bert-base-uncased') tokenizer I was using would subword tokenize entities it as not familiar with. This poses a major problem and rendered the labeling process innefective. \n",
    "\n",
    "To address this issue, in this notebook I train two new tokenizers on the combined corpus of both conll2003 dataset and the wikipedia toxic comments dataset. One that includes entire conll2003 and wiki dataset. And another that include the enitre conll2003 and only the toxic examples from the wiki set. These tokenizers will be used to fine-tune the base distilbert model.\n",
    "\n",
    "The reason that I must fine-tune the entire model again is because using a different tokenizer majorly impacts the performance of the model. This is because the input ids, the numbers the model recieves as input, will be entirely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer training Strategy:\n",
    "The first step is to gather up the text data from the conll2003 and the wiki set into a single corpus.txt file\n",
    "Then the training is simple from there and can be done in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "conll2003 = datasets.load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am converting the train, test, and val sets to pandas dfs for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...</td>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...</td>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens   \n",
       "0  0  [EU, rejects, German, call, to, boycott, Briti...  \\\n",
       "1  1                                 [Peter, Blackburn]   \n",
       "2  2                             [BRUSSELS, 1996-08-22]   \n",
       "3  3  [The, European, Commission, said, on, Thursday...   \n",
       "4  4  [Germany, 's, representative, to, the, Europea...   \n",
       "\n",
       "                                            pos_tags   \n",
       "0                [22, 42, 16, 21, 35, 37, 16, 21, 7]  \\\n",
       "1                                           [22, 22]   \n",
       "2                                           [22, 11]   \n",
       "3  [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...   \n",
       "\n",
       "                                          chunk_tags   \n",
       "0                [11, 21, 11, 12, 21, 22, 11, 12, 0]  \\\n",
       "1                                           [11, 12]   \n",
       "2                                           [11, 12]   \n",
       "3  [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0                        [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "1                                             [1, 2]  \n",
       "2                                             [5, 0]  \n",
       "3  [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...  \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "#extract out the train, validation, and test sets --- they are in the format of a dataset sequences\n",
    "train, validation, test = conll2003[\"train\"], conll2003[\"validation\"], conll2003[\"test\"]\n",
    "\n",
    "#convert the datasets to pandas dataframes\n",
    "train, validation, test = [x.to_pandas() for x in [train, validation, test]]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am converting the df into a list of white space split tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this results int a list of numpy arrays of lists of tokens [np.array(['a', 'b'], ['c', 'd'])]\n",
    "train_tokens, test_tokens, val_tokens = [x[\"tokens\"].to_list() for x in [train, test, validation]]\n",
    "\n",
    "#map conversion of token examples to a list of lists to all datasets --- result: [['a', 'b'], ['c', 'd']]\n",
    "train_tokens, test_tokens, val_tokens = [list(map(list, x)) for x in [train_tokens, test_tokens, val_tokens]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally I am concatenating them to a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all lists into one list\n",
    "all_tokens_conll2003 = train_tokens + test_tokens + val_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to do the same thign with the wikipedia comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic   \n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0  \\\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir_path = r\"C:\\Users\\hunte\\OneDrive\\Documents\\Coding Projects\\Bot_Discord_Proj\\Original_Folder\\data\"\n",
    "paths = [os.path.join(dir_path, data_split) for data_split in [\"train.csv\", \"test.csv\"]] #path to train.csv\n",
    "\n",
    "train, test = [pandas.read_csv(path) for path in paths]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am converting the pandas dfs for both datasets into lists of white space seperated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples form conll2003: 20744\n",
      "number of examples from wiki set: 312735\n",
      "\n",
      "number of non-unique tokens from conll2003: 301418\n",
      "number of non-unique tokens from wiki set: 20171453\n",
      "\n",
      "total non-unique tokens: 20,472,871\n"
     ]
    }
   ],
   "source": [
    "#extract \"comment_text\" out from df and convert to list of strings\n",
    "train_text, test_text = [x[\"comment_text\"].to_list() for x in [train, test]]\n",
    "\n",
    "#map conversions of lists of strings ----> lists of white space split tokens\n",
    "train_tokens, test_tokens = [list(map(str.split, x)) for x in [train_text, test_text]]\n",
    "\n",
    "#concatenate the train and test tokens\n",
    "all_tokens_wiki = train_tokens + test_tokens\n",
    "\n",
    "print(f'number of examples form conll2003: {len(all_tokens_conll2003)}') \n",
    "print(f'number of examples from wiki set: {len(all_tokens_wiki)}\\n')\n",
    "\n",
    "#sum lengths of examples across datasets\n",
    "non_unique_tokens_conll2003 = sum([len(example) for example in all_tokens_conll2003])\n",
    "non_unique_tokens_wiki = sum([len(example) for example in all_tokens_wiki])\n",
    "\n",
    "print(f'number of non-unique tokens from conll2003: {non_unique_tokens_conll2003}')\n",
    "print(f'number of non-unique tokens from wiki set: {non_unique_tokens_wiki}\\n')\n",
    "print(f'total non-unique tokens: {non_unique_tokens_conll2003 + non_unique_tokens_wiki:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am combining the two lists of tokens together into a single list of lists of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the two datasets\n",
    "all_tokens = all_tokens_conll2003 + all_tokens_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333479"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the number of unique tokens I am going to convert all_tokens to a pandas series and apply .unique() to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972712"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the list of lists\n",
    "tokens_list = [token for example in all_tokens for token in example]\n",
    "\n",
    "# Convert the flattened list to a Series\n",
    "s = pd.Series(tokens_list)\n",
    "\n",
    "# Find unique values\n",
    "unique_values = s.unique()\n",
    "\n",
    "len(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are nearly 1,000,000 unique tokens. During training, the tokenizer will judge which words and subwords to include in the vocabulary based on frequency. I am a bit concerned that perhaps some toxic words will be subword tokenized or shafted into the [UNK] token.\n",
    "\n",
    "I will keep this in mind. I am thinking that I will also create a corupus that has a greater emphasis on the toxic vocab from the wiki set and foregoing the clean examples. \n",
    "\n",
    "Below I am going to write all_tokens to a corpus.txt file woth one example per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hunte\\\\OneDrive\\\\Documents\\\\Coding Projects\\\\Bot_Discord_Proj\\\\NER_model'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conll_wiki_corpus.txt\", \"wb\") as f:\n",
    "    for i in range(len(all_tokens)):\n",
    "        #encode joined tokens to utf-8 and write to file\n",
    "        example = (\" \".join(all_tokens[i]) + \"\\n\").encode(\"utf-8\")\n",
    "        f.write(example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Tokenizer with Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer training process will determine what tokens from the corpus are included in its standard vocabulary, which to subword tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\hunte\\\\OneDrive\\\\Documents\\\\Coding Projects\\\\Bot_Discord_Proj\\\\NER_model\\\\conll_wiki_tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "#path to corpus \n",
    "corpus_path = os.path.join(os.getcwd(), \"conll_wiki_corpus.txt\")\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(\n",
    "    files=[corpus_path],  \n",
    "    vocab_size=30000,  # 30,000 is a standard value for BERT\n",
    "    min_frequency=2,  # Minimum frequency for a token to be included in vocab\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=1000,  # Limit the alphabet size\n",
    "    wordpieces_prefix=\"##\"  # Prefix for subwords\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_model(os.getcwd(), \"conll_wiki_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Tokenizer\n",
    "I will also, quickly, run through the entire process of constructing the corpus and training the tokenizer again, but this time without the non-toxic examples form the wiki dataset. This is to address my concern that the toxic words from the wiki set will be lost to subword tokenization by more frequent friendly words from that set. This concern will be ab tested by fine tuning a model with each tokenizer.\n",
    "\n",
    "I won't explain the code below beyond the comments since it is the same process as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20744"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens_conll2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples in all_tokens: 36969\n"
     ]
    }
   ],
   "source": [
    "#load wiki set\n",
    "dir_path = r\"C:\\Users\\hunte\\OneDrive\\Documents\\Coding Projects\\Bot_Discord_Proj\\Original_Folder\\data\"\n",
    "path = os.path.join(dir_path, \"train.csv\") #path to train.csv\n",
    "\n",
    "#only train contains labels\n",
    "train = pandas.read_csv(path)\n",
    "\n",
    "toxic_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "#if any of the labels are 1, then the example is toxic and isolate it into a new dataframe\n",
    "tox_df = train[ train[toxic_cols].any(axis=1).astype(int).eq(1) ]\n",
    "\n",
    "#extract \"comment_text\" out from df and convert to list of strings\n",
    "train_text = tox_df[\"comment_text\"].to_list()\n",
    "\n",
    "#map conversions of lists of strings ----> lists of white space split tokens\n",
    "train_tokens = list(map(str.split, train_text))\n",
    "\n",
    "#concatenate the two datasets\n",
    "all_tokens = all_tokens_conll2003 + train_tokens\n",
    "print(f'Examples in all_tokens: {len(all_tokens)}')\n",
    "\n",
    "#write corpus to file\n",
    "with open(\"conll_toxic_wiki_corpus.txt\", \"wb\") as f:\n",
    "    for i in range(len(all_tokens)):\n",
    "        #encode joined tokens to utf-8 and write to file\n",
    "        example = (\" \".join(all_tokens[i]) + \"\\n\").encode(\"utf-8\")\n",
    "        f.write(example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\hunte\\\\OneDrive\\\\Documents\\\\Coding Projects\\\\Bot_Discord_Proj\\\\NER_model\\\\conll_toxic_wiki_tokenizer-vocab.txt']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "#path to corpus \n",
    "corpus_path = os.path.join(os.getcwd(), \"conll_toxic_wiki_corpus.txt\")\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(\n",
    "    files=[corpus_path],  \n",
    "    vocab_size=30000,  # 30,000 is a standard value for BERT\n",
    "    min_frequency=2,  # Minimum frequency for a token to be included in vocab\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=1000,  # Limit the alphabet size\n",
    "    wordpieces_prefix=\"##\"  # Prefix for subwords\n",
    ")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_model(os.getcwd(), \"conll_toxic_wiki_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
